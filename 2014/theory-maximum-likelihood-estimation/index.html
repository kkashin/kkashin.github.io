<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="apple-mobile-web-app-capable" content="yes"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title> Theory of Maximum Likelihood Estimation | Konstantin Kashin </title> <meta name="description" content=" Konstantin Kashin is an Engineering Manager at Meta, interested in AI, causal inference, experimentation, and optimization. "> <meta name="keywords" content="artificial intelligence, AI, data science, engineering, causal inference, A/B testing, experimentation, optimization"> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <!-- Social: Facebook / Open Graph --> <meta property="og:type" content="article"> <meta property="article:author" content="Konstantin Kashin"> <meta property="article:section" content="blog"> <meta property="article:tag" content=""> <meta property="article:published_time" content="2014-04-29 00:00:00 -0400"> <meta property="og:url" content="http://kkashin.com/2014/theory-maximum-likelihood-estimation/"> <meta property="og:title" content=" Theory of Maximum Likelihood Estimation | Konstantin Kashin "> <meta property="og:image" content="http://kkashin.com"> <meta property="og:description" content=" Konstantin Kashin is an Engineering Manager at Meta, interested in AI, causal inference, experimentation, and optimization. "> <meta property="og:site_name" content="Konstantin Kashin"> <meta property="og:locale" content="en_US"> <!-- Social: Twitter --> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:site" content=""> <meta name="twitter:title" content=" Theory of Maximum Likelihood Estimation | Konstantin Kashin "> <meta name="twitter:description" content=" Konstantin Kashin is an Engineering Manager at Meta, interested in AI, causal inference, experimentation, and optimization. "> <meta name="twitter:image:src" content="http://kkashin.com"> <!-- Social: Google+ / Schema.org --> <meta itemprop="name" content=" Theory of Maximum Likelihood Estimation | Konstantin Kashin "> <meta itemprop="description" content=" Konstantin Kashin is an Engineering Manager at Meta, interested in AI, causal inference, experimentation, and optimization. "> <meta itemprop="image" content="http://kkashin.com"> <!-- rel prev and next --> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="shortcut icon" type="image/png" href="/assets/images/favicon-32x32.png"> <!-- Canonical link tag --> <link rel="canonical" href="http://kkashin.com/2014/theory-maximum-likelihood-estimation/"> <link rel="alternate" type="application/rss+xml" title="Konstantin Kashin" href="http://kkashin.com/feed.xml"> <script type="text/javascript"> var disqus_shortname = ''; var _gaq = _gaq || []; _gaq.push(['_setAccount', 'UA-29054835-1']); _gaq.push(['_trackPageview']); (function() { var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true; ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); })(); </script> <!-- MathJax scripts --> <script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], displayMath: [['\\[','\\]'], ['$$','$$']]}}); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script> </head> <body> <main class="wrapper"> <header class="site-header"> <nav class="nav"> <div class="container"> <h1 class="logo"><a href="/">Konstantin Kashin<span></span></a></h1> <ul class="navbar"> <li><a href="/about">about</a></li> <li><a href="/projects">projects</a></li> <li><a href="/data">data</a></li> </ul> </div> </nav> </header> <article class="post container" itemscope itemtype="http://schema.org/BlogPosting"> <header class="post-header"> <h1 class="post-title" itemprop="name headline">Theory of Maximum Likelihood Estimation</h1> <p class="post-meta"><time datetime="2014-04-29T00:00:00-04:00" itemprop="datePublished">Apr 29, 2014</time></p> </header> <div class="post-content" itemprop="articleBody"> <p>I’ve finally updated and uploaded a detailed note on maximum likelihood estimation, based in part on material I taught in Gov 2001. It is available in full <a href="/assets/notes/Maximum_Likelihood_Estimation.pdf">here</a>. <br /> <br /> To summarize the note without getting into too much math, let’s first define the likelihood as proportional to the joint probability of the data conditional on the parameter of interest ($\theta$): \(L(\theta|\mathbf{x}) \propto f(\mathbf{x}|\theta) = \prod\limits_{i=1}^n f(x_i|\theta)\) The maximum likelihood estimate (MLE) of $\theta$ is the value of $\theta$ in the parameter space $\Omega$ that maximizes the likelihood function: \(\hat{\theta}_{MLE} = \max_{\theta \in \Omega} L(\theta|\mathbf{x}) = \max_{\theta \in \Omega} \prod\limits_{i=1}^n f(x_i|\theta)\)</p> <p>This turns out to be equivalent to maximizing the log-likelihood function (which is often simpler): \(\hat{\theta}_{MLE} = \max_{\theta \in \Omega} \log L(\theta|\mathbf{x}) = \max_{\theta \in \Omega} \ell (\theta|\mathbf{x}) = \max_{\theta \in \Omega} \sum\limits_{i=1}^n \log (f(x_i|\theta))\)</p> <p>One can find the MLE either analytically (using calculus) or numerically (by using R or another program). <br /> <br /> <br /></p> <h3 id="a-simple-example">A Simple Example</h3> <p>Suppose that we want to visualize the log-likelihood curve for data drawn from a Poisson distribution with an unknown parameter $\lambda$. The data we observe is {2,1,1,4,4,2,1,2,1,2}. In R, we can do this quite simply as: <br /> <br /></p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">my.data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">4</span><span class="p">,</span><span class="m">4</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">pois.ll</span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="nf">return</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">dpois</span><span class="p">(</span><span class="n">my.data</span><span class="p">,</span><span class="n">lambda</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="n">log</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)))</span><span class="w">
</span><span class="n">pois.ll</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Vectorize</span><span class="p">(</span><span class="n">pois.ll</span><span class="p">)</span><span class="w">
</span><span class="n">curve</span><span class="p">(</span><span class="n">pois.ll</span><span class="p">,</span><span class="w"> </span><span class="n">from</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="n">to</span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">lambda</span><span class="p">),</span><span class="n">ylab</span><span class="o">=</span><span class="s2">"Log-Likelihood"</span><span class="p">)</span></code></pre></figure> <p><br /> <br /> We already know (based on analytic solutions) that the MLE for $\lambda$ in a Poisson distribution is just the sample mean, which comes out to 2 in this case. Thus, we can mark it on the log-likelihood curve to produce the following graph:</p> <div class="post-image"> <a href="/assets/images/blog/poisson_ll.jpg"><img alt="poisson likelihood curve" src="/assets/images/blog/poisson_ll.jpg" height="320" width="320" /></a> </div> <p>If we wanted to maximize the log-likelihood in R (on the parameter space [0,100], chosen because it’s sufficiently wide to encompass the MLE), we could have done:</p> <p><br /></p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">opt</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">optimize</span><span class="p">(</span><span class="n">pois.ll</span><span class="p">,</span><span class="w"> </span><span class="n">interval</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">100</span><span class="p">),</span><span class="n">maximum</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">opt</span><span class="o">$</span><span class="n">maximum</span><span class="w"> </span><span class="c1"># gives MLE</span><span class="w">
</span><span class="n">opt</span><span class="o">$</span><span class="n">objective</span><span class="w"> </span><span class="c1"># gives value of log-likelihood at MLE</span></code></pre></figure> <p><br /> R confirms our analytic solution. <br /> <br /> <br /></p> <h3 id="theory-of-maximum-likelihood-estimation">Theory of Maximum Likelihood Estimation</h3> <p>Why do we use maximum likelihood estimation? It turns out that subject to regularity conditions the following properties hold for the MLE:</p> <ul> <li> <p>Consistency: as sample size ($n$) increases, the MLE ($\hat{\theta}_{MLE}$) converges to the true parameter, $\theta_0$. \(\hat{\theta}_{MLE} \overset{p}{\longrightarrow} \theta_0\)</p> </li> <li> <p>Normality: As sample size ($n$) increases, the MLE is normally distributed with a mean equal to the true parameter ($\theta_0$) and the variance equal to the inverse of the expected sample Fisher information at the true parameter. However, using the consistency property of the MLE, we can use the inverse of the observed sample Fisher information evaluated at the MLE, denoted as $\mathcal{J}<em>n(\hat{\theta}</em>{MLE})$ to approximate the variance. The observed sample Fisher information is the negation of the second derivative of the log-likelihood curve. \(\hat{\theta}_{MLE} \sim \mathcal{N} \left(\theta_0, \Big(\underbrace{- \Big( \dfrac{\partial^2 \ell(\theta|\mathbf{x})}{\partial \theta^2} \Big|_{\theta=\hat{\theta}_{MLE}} \Big)}_{\mathcal{J}_n(\hat{\theta}_{MLE})} \Big)^{-1} \right)\)</p> </li> <li> <p>Efficiency: maximum likelihood estimation generally provides the lowest variance as sample size increases.</p> </li> </ul> </div> <div class="post-nav"> <a rel="prev" class="prev" href="/2013/network-visualization-with-d3js/">&larr; Network Visualization with D3.js</a> <a rel="next" class="next" href="/2014/databits/">Data Visualization & Databits &rarr;</a> </div> </article> <footer class="site-footer"> <div class="container"> <small class="pull-left">&copy;2024 Konstantin Kashin. Made with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>.</small> <small class="pull-right"> <ul class="contact-list"> <li> <a href="https://linkedin.com/in/konstantin-kashin-8939a492"><span class="icon"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>LinkedIn icon</title><path fill="#828282" d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg> </span></a> </li> <li> <a href="https://github.com/kkashin"><span class="icon icon--github"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub icon</title><path fill="#828282" d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg> </span></a> </li> <li> <a href="https://twitter.com/@kostyakashin"><span class="icon icon--twitter"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Twitter icon</title><path fill="#828282" d="M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717 0-4.92 2.203-4.92 4.917 0 .39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475 0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314 0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39 0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z"/></svg> </span></a> </li> </ul></small> </div> </footer> </main> </body> </html>
